# Replication-Package
# üìö On the Study of ML Cloud Service Misuses: An Industrial Perspective

Welcome to the replication package of our paper **On the Study of ML Cloud Service Misuses: An Industrial Perspective**! üöÄ This repository compiles all results from the systematic review of literature, mining of GitHub repositories, and analysis of survey responses. It aims to uncover trends, patterns, and insights across multiple software development and research projects.

## üìÇ Repository Contents

This repository contains the following resources:

### 1. üìÑ **Papers Collected from the Systematic Literature Review**
   - A collection of research papers and publications gathered during the systematic literature review. Below are the references for the paper. 
      You can also find them [here](https://github.com/ml-service-misuses/Replication_Package/blob/main/Papers%20set.pdf)

      - WAN, Chengcheng, LIU, Shicheng, HOFFMANN, Henry, et al. Are machine learning cloud APIs used correctly? In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). 2021.
      
      - CAO, Junming, CHEN, Bihuan, SUN, Chao, et al. Understanding performance problems in deep learning systems. In: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering.
      
      - WASHIZAKI, Hironori, KHOMH, Foutse, GU√âH√âNEUC, Yann-Ga√´l, et al. Software-engineering design patterns for machine learning applications. Computer, 2022, vol. 55, no 3, p. 30-39.
      
      - O'BRIEN, David, BISWAS, Sumon, IMTIAZ, Sayem, et al. 23 shades of self-admitted technical debt: An empirical study on machine learning software. In: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2022. p. 734-746.
      
      - YANG, Chenyang, BROWER-SINNING, Rachel A., LEWIS, Grace, et al. Data leakage in notebooks: Static detection and better processes. In: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. 2022. p. 1-12.
      
      - WANG, Jun, XIAO, Guanping, ZHANG, Shuai, et al. Compatibility Issues in Deep Learning Systems: Problems and Opportunities. In: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2023. p. 476-488.
      
      - DILHARA, Malinda, KETKAR, Ameya, SANNIDHI, Nikhith, et al. Discovering repetitive code changes in Python ML systems. In: Proceedings of the 44th International Conference on Software Engineering. 2022. p. 736-748.
      
      - NAHAR, Nadia, ZHOU, Shurui, LEWIS, Grace, et al. Collaboration challenges in building ML-enabled systems: Communication, documentation, engineering, and process. In: Proceedings of the 44th International Conference on Software Engineering. 2022. p. 413-425.
      
      - NAHAR, Nadia, ZHANG, Haoran, LEWIS, Grace, et al. A meta-summary of challenges in building products with ML components‚Äìcollecting experiences from 4758+ practitioners. In: 2023 IEEE/ACM 2nd International Conference on AI Engineering‚ÄìSoftware Engineering for AI (CAIN). IEEE, 2023. p. 171-183.
      
      - YANG, Jingbo, REN, Jian, et WU, Wenjun. API misuse detection method based on transformer. In: 2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS). IEEE, 2022. p. 958-969.
      
      - SERBAN, Alex, VAN DER BLOM, Koen, HOOS, Holger, et al. Adoption and effects of software engineering best practices in machine learning. In: Proceedings of the 14th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM). 2020. p. 1-12.
      
      - VAN OORT, Bart, CRUZ, Lu√≠s, LONI, Babak, et al. "Project smells" experiences in analysing the software quality of ML projects with mllint. In: Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice. 2022. p. 211-220.
      
      - WAN, Chengcheng, LIU, Shicheng, XIE, Sophie, et al. Automated testing of software that uses machine learning APIs. In: Proceedings of the 44th International Conference on Software Engineering. 2022. p. 212-224.
      - LU, Qinghua, ZHU, Liming, XU, Xiwei, et al. Responsible AI pattern catalogue: A collection of best practices for AI governance and engineering. ACM Computing Surveys, 2024, vol. 56, no 7, p. 1-35.  
      
      - YANG, Jingbo, REN, Jian, et WU, Wenjun. API Misuse Detection Method Based on Transformer. In: 2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS). IEEE, 2022. p. 958-969.  
      
      - HARDT, Michaela, CHEN, Xiaoguang, CHENG, Xiaoyi, et al. Amazon SageMaker Clarify: Machine learning bias detection and explainability in the cloud. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021. p. 2974-2983.  
      
      - RAVINDRANATHAN, Manya K., VADIVU, D. Sendil, et RAJAGOPALAN, Narendran. Cloud-Driven Machine Learning with AWS: A Comprehensive Review of Services. In: 2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE). IEEE, 2024. p. 1-8.  
      
      - MEHRABI, Ninareh, MORSTATTER, Fred, SAXENA, Nripsuta, et al. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 2021, vol. 54, no 6, p. 1-35.  
      
      - CABRAL, Raphael, KALINOWSKI, Marcos, BALDASSARRE, Maria Teresa, et al. Investigating the Impact of SOLID Design Principles on Machine Learning Code Understanding. In: Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI. 2024. p. 7-17.  
      
      - ZHANG, Huaifeng, ALHANAHNAH, Mohannad, AHMED, Fahmi Abdulqadir, et al. Machine Learning Systems are Bloated and Vulnerable. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 2024, vol. 8, no 1, p. 1-30.  
      
      - ZHANG, Haiyin, CRUZ, Lu√≠s, et VAN DEURSEN, Arie. Code smells for machine learning applications. In: Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI. 2022. p. 217-228.  
      
      - CARDOZO, Nicol√°s, DUSPARIC, Ivana, et CABRERA, Christian. Prevalence of code smells in reinforcement learning projects. In: 2023 IEEE/ACM 2nd International Conference on AI Engineering - Software Engineering for AI (CAIN). IEEE, 2023. p. 37-42.  
      
      - WOLF, Christine T. et PAINE, Drew. Sensemaking practices in the everyday work of AI/ML software engineering. In: Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops. 2020. p. 86-92.  
      
      - WASHIZAKI, Hironori, TAKEUCHI, Hironori, KHOMH, Foutse, et al. Practitioners‚Äô insights on machine-learning software engineering design patterns: a preliminary study. In: 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2020. p. 797-799.  
      
      - WASHIZAKI, Hironori, UCHIDA, Hiromu, KHOMH, Foutse, et al. Studying software engineering patterns for designing machine learning systems. In: 2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP). IEEE, 2019. p. 49-495.  
      
      - RAHMAN, Mafizur, CHY, Md Showkat Hossain, et SAHA, Swapnil. A systematic review on software design patterns in today's perspective. In: 2023 IEEE 11th International Conference on Serious Games and Applications for Health (SeGAH). IEEE, 2023. p. 1-8.  
      
      - DUR√ÅN, Francisco, MART√çNEZ-FERN√ÅNDEZ, Silverio, MARTINEZ, Matias, et al. Identifying architectural design decisions for achieving green ML serving. In: Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI. 2024. p. 18-23.  
      
      - BERNARDO, Jo√£o Helis, DA COSTA, Daniel Alencar, MEDEIROS, S√©rgio Queiroz de, et al. How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions. In: Proceedings of the 21st International Conference on Mining Software Repositories. 2024. p. 665-676.  
      
      - YANG, Sheng, KHULLER, Samir, CHOUDHARY, Sunav, et al. Scheduling ML training on unreliable spot instances. In: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion. 2021. p. 1-8.  
      
      - POLYZOTIS, Neoklis, ZINKEVICH, Martin, ROY, Sudip, et al. Data validation for machine learning. Proceedings of Machine Learning and Systems, 2019, vol. 1, p. 334-347.  
      
      - ANTUNES, Nuno, BALBY, Leandro, FIGUEIREDO, Flavio, et al. Fairness and transparency of machine learning for trustworthy cloud services. In: 2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W). IEEE, 2018. p. 188-193.  
      
      - XU, Xiangzhe, LIU, Hongyu, TAO, Guanhong, et al. Checkpointing and deterministic training for deep learning. In: Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI. 2022. p. 65-76.  
      
      - ROJAS, Elvis, KAHIRA, Albert Njoroge, MENESES, Esteban, et al. A study of checkpointing in large scale training of deep neural networks. arXiv preprint arXiv:2012.00825, 2020.  
      
      - WEERTS, Hilde JP, MUELLER, Andreas C., et VANSCHOREN, Joaquin. Importance of tuning hyperparameters of machine learning algorithms. arXiv preprint arXiv:2007.07588, 2020.  
      
      - VICTORIA, A. Helen et MARAGATHAM, Ganesh. Automatic tuning of hyperparameters using Bayesian optimization. Evolving Systems, 2021, vol. 12, no 1, p. 217-223.  
      
      - WANG, Chenyu, YANG, Zhou, LI, Ze Shi, et al. Quality assurance for artificial intelligence: A study of industrial concerns, challenges and best practices. arXiv preprint arXiv:2402.16391, 2024.  
      
      - BOVA, Carol, JAFFARIAN, Carol, CRAWFORD, Sybil, et al. Intervention fidelity: monitoring drift, providing feedback, and assessing the control condition. Nursing Research, 2017, vol. 66, no 1, p. 54-59.  
      
      - MALLICK, Ankur, HSIEH, Kevin, ARZANI, Behnaz, et al. Matchmaker: Data drift mitigation in machine learning for large-scale systems. Proceedings of Machine Learning and Systems, 2022, vol. 4, p. 77-94.  
      
      - Naser, M. Z., and Amir Alavi. Insights into performance fitness and error metrics for machine learning. arXiv preprint arXiv:2006.00887 (2020).  

### 2. üíª **GitHub Repositories Mined**
   - A curated collection of GitHub repositories related to the research topics: ML services and antipatterns.
   - These repositories were mined using Python scripts based on keywords found in the README/description of each repository.
   - In-depth information on the GitHub repositories, including: descriptions, contributors, number of stars, and forks.

### 3. üìä **Results of the Analysis of GitHub Projects**
   - This analysis identifies ML service misuses across the projects: [here](https://github.com/ml-service-misuses/Replication_Package/blob/main/ML%20Service%20misuses-occurrence.pdf).
     
     You can find the detailed analysis [here](https://github.com/ml-service-misuses/Replication_Package/blob/main/Detected%20Misuses.xlsx). 
 ![Alt text](https://github.com/ml-service-misuses/Replication_Package/blob/main/occurrence.png)


### 4. üìã **Pilot Interviews Responses (Anonymized)**
   - Responses collected from our pilot interviews.
   - The data has been anonymized to protect the privacy of participants.
   - Provides valuable insights into participants' feedback on the survey, including remarks on its length, consistency, and any duplicate questions: [here](https://github.com/ml-service-misuses/Replication_Package/blob/main/Pilot%20Interviews%20(r%C3%A9ponses).csv)

### 5. üìã **Survey Responses (Anonymized)**
   - Responses collected from our survey.
   - The data has been anonymized to protect the privacy of participants.
   - Provides valuable insights into developer perspectives on ML service misuses: [here](https://github.com/ml-service-misuses/Replication_Package/blob/main/ML%20practitioners%20Survey%20(reponses).csv)
## üõ† Technologies & Tools Used

- **Data Mining**: Python, GitHub API, StackOverflow API
- **Data Analysis**: Python, Understand tool
- **Survey Platform**: Google Forms
- **Literature Review Tools**: Google Scholar

## üîó Online Catalog
Explore the catalog [here](https://ml-service-misuses.github.io/Catalog-Proposal/)

Thank you for visiting! üéâ
